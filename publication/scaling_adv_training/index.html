<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Fira+Sans&family=Fira+Code&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Fira+Sans&family=Fira+Code&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.bba687d229709d201099e637a1c464c1.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Edoardo Debenedetti"><meta name=description content="The last six years have witnessed significant progress in adversarially robust deep learning. As evidenced by the CIFAR-10 dataset category in RobustBench benchmark, the accuracy under ℓ∞ adversarial perturbations improved from 44% in Madry et al. (2018) to 71% in Peng et al. (2023). Although impressive, existing state-of-the-art is still far from satisfactory. It is further observed that best-performing models are often very large models adversarially trained by industrial labs with significant computational budgets. In this paper, we aim to understand: &#34;how much longer can computing power drive adversarial robustness advances?&#34; To answer this question, we derive *scaling laws for adversarial robustness* which can be extrapolated in the future to provide an estimate of how much cost we would need to pay to reach a desired level of robustness. We show that increasing the FLOPs needed for adversarial training does not bring as much advantage as it does for standard training in terms of performance improvements. Moreover, we find that some of the top-performing techniques are difficult to exactly reproduce, suggesting that they are not robust enough for minor changes in the training setup. Our analysis also uncovers potentially worthwhile directions to pursue in future research. Finally, we make our benchmarking framework (built on top of `timm`) publicly available to facilitate future analysis in efficient robust deep learning."><link rel=alternate hreflang=en-us href=https://edoardo.science/publication/scaling_adv_training/><link rel=canonical href=https://edoardo.science/publication/scaling_adv_training/><link rel=manifest href=/manifest.webmanifest><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@edoardo_debe"><meta property="twitter:creator" content="@edoardo_debe"><meta property="twitter:image" content="https://edoardo.science/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Edoardo Debenedetti"><meta property="og:url" content="https://edoardo.science/publication/scaling_adv_training/"><meta property="og:title" content="Scaling Compute Is Not All You Need for Adversarial Robustness | Edoardo Debenedetti"><meta property="og:description" content="The last six years have witnessed significant progress in adversarially robust deep learning. As evidenced by the CIFAR-10 dataset category in RobustBench benchmark, the accuracy under ℓ∞ adversarial perturbations improved from 44% in Madry et al. (2018) to 71% in Peng et al. (2023). Although impressive, existing state-of-the-art is still far from satisfactory. It is further observed that best-performing models are often very large models adversarially trained by industrial labs with significant computational budgets. In this paper, we aim to understand: &#34;how much longer can computing power drive adversarial robustness advances?&#34; To answer this question, we derive *scaling laws for adversarial robustness* which can be extrapolated in the future to provide an estimate of how much cost we would need to pay to reach a desired level of robustness. We show that increasing the FLOPs needed for adversarial training does not bring as much advantage as it does for standard training in terms of performance improvements. Moreover, we find that some of the top-performing techniques are difficult to exactly reproduce, suggesting that they are not robust enough for minor changes in the training setup. Our analysis also uncovers potentially worthwhile directions to pursue in future research. Finally, we make our benchmarking framework (built on top of `timm`) publicly available to facilitate future analysis in efficient robust deep learning."><meta property="og:image" content="https://edoardo.science/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2024-10-20T12:22:26+00:00"><meta property="article:modified_time" content="2024-05-11T12:22:26+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://edoardo.science/publication/scaling_adv_training/"},"headline":"Scaling Compute Is Not All You Need for Adversarial Robustness","datePublished":"2024-10-20T12:22:26Z","dateModified":"2024-05-11T12:22:26Z","author":{"@type":"Person","name":"Edoardo Debenedetti"},"publisher":{"@type":"Organization","name":"Edoardo Debenedetti","logo":{"@type":"ImageObject","url":"https://edoardo.science/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_3.png"}},"description":"The last six years have witnessed significant progress in adversarially robust deep learning. As evidenced by the CIFAR-10 dataset category in RobustBench benchmark, the accuracy under ℓ∞ adversarial perturbations improved from 44% in Madry et al. (2018) to 71% in Peng et al. (2023). Although impressive, existing state-of-the-art is still far from satisfactory. It is further observed that best-performing models are often very large models adversarially trained by industrial labs with significant computational budgets. In this paper, we aim to understand: \"how much longer can computing power drive adversarial robustness advances?\" To answer this question, we derive *scaling laws for adversarial robustness* which can be extrapolated in the future to provide an estimate of how much cost we would need to pay to reach a desired level of robustness. We show that increasing the FLOPs needed for adversarial training does not bring as much advantage as it does for standard training in terms of performance improvements. Moreover, we find that some of the top-performing techniques are difficult to exactly reproduce, suggesting that they are not robust enough for minor changes in the training setup. Our analysis also uncovers potentially worthwhile directions to pursue in future research. Finally, we make our benchmarking framework (built on top of `timm`) publicly available to facilitate future analysis in efficient robust deep learning."}</script><title>Scaling Compute Is Not All You Need for Adversarial Robustness | Edoardo Debenedetti</title><script async defer src=https://scripts.simpleanalyticscdn.com/latest.js></script><noscript><img src=https://queue.simpleanalyticscdn.com/noscript.gif alt referrerpolicy=no-referrer-when-downgrade></noscript></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=d98ad7fbe05a0a136be6c903cfd45a6a><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#><span>Home</span></a></li><li class=nav-item><a class="nav-link active" href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/files/cv.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>Scaling Compute Is Not All You Need for Adversarial Robustness</h1><div class=article-metadata><div><span>Edoardo Debenedetti</span>, <span>Zishen Wan</span>, <span>Maksym Andriushchenko</span>, <span>Vikash Sehwag</span>, <span>Kshitij Bhardwaj</span>, <span>Bhavya Kailkhura</span></div><span class=article-date>May, 2024</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=https://arxiv.org/abs/2312.13131 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/scaling_adv_training/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header" href=http://github.com/dedeswim/timm-adv-training/ target=_blank rel=noopener>Code</a></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>The last six years have witnessed significant progress in adversarially robust deep learning. As evidenced by the CIFAR-10 dataset category in RobustBench benchmark, the accuracy under ℓ∞ adversarial perturbations improved from 44% in Madry et al. (2018) to 71% in Peng et al. (2023). Although impressive, existing state-of-the-art is still far from satisfactory. It is further observed that best-performing models are often very large models adversarially trained by industrial labs with significant computational budgets. In this paper, we aim to understand: &ldquo;how much longer can computing power drive adversarial robustness advances?&rdquo; To answer this question, we derive <em>scaling laws for adversarial robustness</em> which can be extrapolated in the future to provide an estimate of how much cost we would need to pay to reach a desired level of robustness. We show that increasing the FLOPs needed for adversarial training does not bring as much advantage as it does for standard training in terms of performance improvements. Moreover, we find that some of the top-performing techniques are difficult to exactly reproduce, suggesting that they are not robust enough for minor changes in the training setup. Our analysis also uncovers potentially worthwhile directions to pursue in future research. Finally, we make our benchmarking framework (built on top of <code>timm</code>) publicly available to facilitate future analysis in efficient robust deep learning.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">ICLR 2024 Workshop on Reliable and Responsible Foundation Models</div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fedoardo.science%2Fpublication%2Fscaling_adv_training%2F&text=Scaling+Compute+Is+Not+All+You+Need+for+Adversarial+Robustness" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fedoardo.science%2Fpublication%2Fscaling_adv_training%2F&t=Scaling+Compute+Is+Not+All+You+Need+for+Adversarial+Robustness" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Scaling%20Compute%20Is%20Not%20All%20You%20Need%20for%20Adversarial%20Robustness&body=https%3A%2F%2Fedoardo.science%2Fpublication%2Fscaling_adv_training%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fedoardo.science%2Fpublication%2Fscaling_adv_training%2F&title=Scaling+Compute+Is+Not+All+You+Need+for+Adversarial+Robustness" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=Scaling+Compute+Is+Not+All+You+Need+for+Adversarial+Robustness%20https%3A%2F%2Fedoardo.science%2Fpublication%2Fscaling_adv_training%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://reddit.com/submit?url=https%3A%2F%2Fedoardo.science%2Fpublication%2Fscaling_adv_training%2F&title=Scaling+Compute+Is+Not+All+You+Need+for+Adversarial+Robustness" target=_blank rel=noopener class=share-btn-reddit aria-label=reddit-alien><i class="fab fa-reddit-alien"></i></a></li></ul></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Powered by the
<a href=https://wowchemy.com/ target=_blank rel=noopener>Wowchemy theme</a> for
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a>, kindly hosted by <a href=https://github.io target=_blank rel=noopener>GitHub Pages</a>, deployed with GitHub Actions.
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div></div><script src=/js/vendor-bundle.min.1d4346c6f7d46c340dc0a9058dd85c13.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/en/js/wowchemy.min.c84202fca2a6efbbecbaf0e8358c1d51.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>