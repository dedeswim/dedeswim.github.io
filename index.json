[{"authors":["edoardo"],"categories":null,"content":"I am a Computer Science Ph.D. Student at ETH Zürich in the Secure and Private AI (SPY) Lab, advised by Florian Tramèr. My interest is in how the current (and future) research about the security and privacy of machine learning systems can be applied to real-world systems. My research is supported by a CYD Doctoral Fellowship awarded by the armasuisse Cyber-Defence Campus.\nPrior to my PhD journey, I earned a Computer Science M.Sc. at EPFL and a Computer Engineering B.Sc. at the Polytechnic University of Turin. I did my Master thesis about the robustness of Vision Transformers supervised by Princeton University’s Prof. Mittal, and I am one of the co-authors and maintainers of RobustBench.\nI previously interned as an SWE intern at Bloomberg LP and as a Research Intern at the armasuisse CYD Campus, supervised by Prof. Humbert.\nMore information can be found on my CV, last updated on 2023/09/26.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f28e12ec1377084c6f96ac549f2be14b","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Computer Science Ph.D. Student at ETH Zürich in the Secure and Private AI (SPY) Lab, advised by Florian Tramèr. My interest is in how the current (and future) research about the security and privacy of machine learning systems can be applied to real-world systems.","tags":null,"title":"Edoardo Debenedetti","type":"authors"},{"authors":["Edoardo Debenedetti","Nicholas Carlini","Florian Tramèr"],"categories":[],"content":"","date":1704555196,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704555196,"objectID":"bf3f4dc9b217df79ec7212471987edae","permalink":"https://edoardo.science/publication/breaking_eggs/","publishdate":"2023-06-06T15:33:16Z","relpermalink":"/publication/breaking_eggs/","section":"publication","summary":"We propose a new real-world oriented metric for black-box decision-based attacks on security-critical systems","tags":[],"title":"Evading Black-box Classifiers Without Breaking Eggs","type":"publication"},{"authors":["Edoardo Debenedetti","Giorgio Severi","Nicholas Carlini","Christopher A. Choquette-Choo","Matthew Jagielski","Milad Nasr","Eric Wallace","Florian Tramèr"],"categories":[],"content":"","date":1694521336,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694521336,"objectID":"5fb0911bac2af8379bcfddc5cb52f369","permalink":"https://edoardo.science/publication/side_channels/","publishdate":"2023-09-12T12:22:16Z","relpermalink":"/publication/side_channels/","section":"publication","summary":"Most current approaches for protecting privacy in machine learning (ML) assume that models exist in a vacuum, when in reality, ML models are part of larger systems that include components for training data filtering, output monitoring, and more. In this work, we introduce privacy side channels: attacks that exploit these system-level components to extract private information at far higher rates than is otherwise possible for standalone models. We propose four categories of side channels that span the entire ML lifecycle (training data filtering, input preprocessing, output post-processing, and query filtering) and allow for either enhanced membership inference attacks or even novel threats such as extracting users' test queries. For example, we show that deduplicating training data before applying differentially-private training creates a side-channel that completely invalidates any provable privacy guarantees. Moreover, we show that systems which block language models from regenerating training data can be exploited to allow exact reconstruction of private keys contained in the training set -- even if the model did not memorize these keys. Taken together, our results demonstrate the need for a holistic, end-to-end privacy analysis of machine learning.","tags":[],"title":"Privacy Side Channels in Machine Learning Systems","type":"publication"},{"authors":["Edoardo Debenedetti","Vikash Sehwag","Prateek Mittal"],"categories":[],"content":"","date":1663687996,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663687996,"objectID":"fbb2b56df6505044500f810a7c069bff","permalink":"https://edoardo.science/publication/light_recipe/","publishdate":"2022-09-20T15:33:16Z","relpermalink":"/publication/light_recipe/","section":"publication","summary":"This paper shows that ViTs are highly suitable for adversarial training to achieve competitive performance and recommends that the community should avoid translating the canonical training recipes in ViTs to robust training and rethink common training choices in the context of adversarial training.","tags":[],"title":"A Light Recipe to Train Robust Vision Transformers","type":"publication"},{"authors":["Edoardo Debenedetti"],"categories":[],"content":"","date":1652358837,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652358837,"objectID":"0f40cf545adf79432be77d9037b7794b","permalink":"https://edoardo.science/publication/thesis/","publishdate":"2022-05-12T12:33:57Z","relpermalink":"/publication/thesis/","section":"publication","summary":"My Master's thesis done at EPFL under the supervision of Princeton's Vikash Sehwag and Prof. Prateek Mittal, and EPFL's Prof. Troncoso. We show that we can obtain state of the art results in adversarial training using Vision Transformers (in particular with XCiT) on ImageNet.","tags":[],"title":"Adversarially Robust Vision Transformers","type":"publication"},{"authors":["Francesco Croce","Maksym Andriushchenko","Vikash Sehwag","Edoardo Debenedetti","Nicolas Flammarion","Mung Chiang","Prateek Mittal","Matthias Hein"],"categories":[],"content":"","date":1617007444,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617007444,"objectID":"2c2bf09e35bb19092bef162197364a73","permalink":"https://edoardo.science/publication/robustbench/","publishdate":"2021-03-29T10:44:04+02:00","relpermalink":"/publication/robustbench/","section":"publication","summary":"As a research community, we are still lacking a systematic understanding of the progress on adversarial robustness which often makes it hard to identify the most promising ideas in training robust models. A key challenge in benchmarking robustness is that its evaluation is often error-prone leading to robustness overestimation. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classification task and introduce restrictions (possibly loosened in the future) on the allowed models and evaluate adversarial robustness with AutoAttack, an ensemble of white- and black-box attacks, which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. To prevent overadaptation of new defenses to AutoAttack, we welcome external evaluations based on adaptive attacks, especially where AutoAttack flags a potential overestimation of robustness. Our leaderboard, hosted at https://robustbench.github.io/, contains evaluations of 120+ models and aims at reflecting the current state of the art in image classification on a set of well-defined tasks in $\\ell_2$- and $\\ell_\\infty$-threat models and on common corruptions, with possible extensions in the future. Additionally, we open-source the library https://github.com/RobustBench/robustbench that provides unified access to 80+ robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze the impact of robustness on the performance on distribution shifts, calibration, out-of-distribution detection, fairness, privacy leakage, smoothness, and transferability. ","tags":[],"title":"RobustBench: A standardized benchmark for adversarial robustness","type":"publication"},{"authors":null,"categories":null,"content":"\r[12/2023 - SaTML 2024 news] Presenting Evading Black-box Classifiers Without Breaking Eggs, and co-organizing the LLMs CTF!\n[06/2023 - New paper: Privacy Side Channels in Machine Learning Systems] We have a new paper, about side-channels in ML systems, i.e., by exploiting components other than the model. Spoiler alert: some of those components, on paper, are meant to improve privacy! Take a look here!\n[06/2023 - New paper: Evading Black-box Classifiers Without Breaking Eggs] We uploaded on arXiv a new paper, where we propose a new real-world oriented metric for black-box decision-based attacks on security-critical systems. Take a look here!\n[11/2022 - A Light Recipe to Train Robust Vision Transformers accepted at SaTML] The paper derived from my master thesis was accepted at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML).\n[09/2022 - New paper: A Light Recipe to Train Robust Vision Transformers] We uploaded on arXiv the paper derived from my Master’s thesis, with additional experiments and insights. Take a look here!\n[08/2022 - I started my PhD] On August 1st, 2022, I started my PhD at ETH Zürich, in the Privacy and Security Lab of Prof. Florian Tramèr.\n[12/05/2022 - I earned my MSc at EPFL!] On April 27th I successfully defended my MSc thesis about Adversarially Robust Vision Transformers! You can read it here. Feel free to contact me if you have any questions about it!\n","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"a0812ae5f3c926fea6faf4472cefc8e2","permalink":"https://edoardo.science/news/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/news/","section":"","summary":"List of news.\r\n","tags":[],"title":"News","type":"page"},{"authors":null,"categories":null,"content":"[12/2023 - SaTML 2024 news] Presenting Evading Black-box Classifiers Without Breaking Eggs, and co-organizing the LLMs CTF!\n[06/2023 - New paper: Privacy Side Channels in Machine Learning Systems] We have a new paper, about side-channels in ML systems, i.e., by exploiting components other than the model. Spoiler alert: some of those components, on paper, are meant to improve privacy! Take a look here!\n[06/2023 - New paper: Evading Black-box Classifiers Without Breaking Eggs] We uploaded on arXiv a new paper, where we propose a new real-world oriented metric for black-box decision-based attacks on security-critical systems. Take a look here!\n[11/2022 - A Light Recipe to Train Robust Vision Transformers accepted at SaTML] The paper derived from my master thesis was accepted at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML).\n[09/2022 - New paper: A Light Recipe to Train Robust Vision Transformers] We uploaded on arXiv the paper derived from my Master’s thesis, with additional experiments and insights. Take a look here!\n[08/2022 - I started my PhD] On August 1st, 2022, I started my PhD at ETH Zürich, in the Privacy and Security Lab of Prof. Florian Tramèr.\n[12/05/2022 - I earned my MSc at EPFL!] On April 27th I successfully defended my MSc thesis about Adversarially Robust Vision Transformers! You can read it here. Feel free to contact me if you have any questions about it!\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4ba0b231c7eeb0b6fffbbe093c76caa6","permalink":"https://edoardo.science/newslist/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/newslist/","section":"","summary":"[12/2023 - SaTML 2024 news] Presenting Evading Black-box Classifiers Without Breaking Eggs, and co-organizing the LLMs CTF!\n[06/2023 - New paper: Privacy Side Channels in Machine Learning Systems] We have a new paper, about side-channels in ML systems, i.","tags":null,"title":"","type":"page"}]