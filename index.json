[{"authors":["edoardo"],"categories":null,"content":"I am a Computer Science Ph.D. Student at ETH Zürich in the Secure and Private AI (SPY) Lab, advised by Florian Tramèr. I am interested in the security and privacy of machine learning systems. Most recently, I have been looking into the security of LLM Agents. Currently, I am also a Research Scientist Intern at Meta, in the GenAI Red Team, working on AI agents security.\nPrior to my PhD, I earned a Computer Science M.Sc. at EPFL and a Computer Engineering B.Sc. at the Polytechnic University of Turin.\nOutside of my studies, I was Student Researcher in the AI Red Team at Google, I interned as a SWE intern at Bloomberg LP, and as a Research Intern at the armasuisse CYD Campus.\nMore information can be found on my CV, last updated on 2025/19/08. For a more recent one, feel free to reach out via email.\nIn my free time, I like all things outdoors, from hiking, to (backcountry) skiing, and sailing.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f28e12ec1377084c6f96ac549f2be14b","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Computer Science Ph.D. Student at ETH Zürich in the Secure and Private AI (SPY) Lab, advised by Florian Tramèr. I am interested in the security and privacy of machine learning systems.","tags":null,"title":"Edoardo Debenedetti","type":"authors"},{"authors":["Edoardo Debenedetti","Jie Zhang","Mislav Balunović","Luca Beurer-Kellner","Marc Fischer","Florian Tramèr"],"categories":[],"content":"","date":1734006977,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734006977,"objectID":"cbd37c81769035b9d9046e8d8df6abd2","permalink":"https://edoardo.science/publication/agentdojo/","publishdate":"2024-10-20T12:36:17Z","relpermalink":"/publication/agentdojo/","section":"publication","summary":"AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data. To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks. We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature. We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner.","tags":[],"title":"AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents","type":"publication"},{"authors":["Edoardo Debenedetti","Javier Rando","Daniel Paleka","Silaghi Fineas Florin","Dragos Albastroiu","Niv Cohen","Yuval Lemberg","Reshmi Ghosh","Rui Wen","Ahmed Salem","Giovanni Cherubin","Santiago Zanella-Beguelin","Robin Schmid","Victor Klemm","Takahiro Miki","Chenhao Li","Stefan Kraft","Mario Fritz","Florian Tramèr","Sahar Abdelnabi","Lea Schönherr"],"categories":[],"content":"","date":1734006708,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734006708,"objectID":"5baf80d0d727a5768af313febf7e5998","permalink":"https://edoardo.science/publication/ctf_report/","publishdate":"2024-10-20T12:31:48Z","relpermalink":"/publication/ctf_report/","section":"publication","summary":"Large language model systems face important security risks from maliciously crafted messages that aim to overwrite the system's original instructions or leak private data. To study this problem, we organized a capture-the-flag competition at IEEE SaTML 2024, where the flag is a secret string in the LLM system prompt. The competition was organized in two phases. In the first phase, teams developed defenses to prevent the model from leaking the secret. During the second phase, teams were challenged to extract the secrets hidden for defenses proposed by the other teams. This report summarizes the main insights from the competition. Notably, we found that all defenses were bypassed at least once, highlighting the difficulty of designing a successful defense and the necessity for additional research to protect LLM systems. To foster future research in this direction, we compiled a dataset with over 137k multi-turn attack chats and open-sourced the platform.","tags":[],"title":"Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition","type":"publication"},{"authors":["Patrick Chao","Edoardo Debenedetti","Alexander Robey","Maksym Andriushchenko","Francesco Croce","Vikash Sehwag","Edgar Dobriban","Nicolas Flammarion","George J. Pappas","Florian Tramer","Hamed Hassani","Eric Wong"],"categories":[],"content":"","date":1733753482,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733753482,"objectID":"5768e7444dbcb445516b78153b51ef0c","permalink":"https://edoardo.science/publication/jailbreakbench/","publishdate":"2024-10-09T14:11:22Z","relpermalink":"/publication/jailbreakbench/","section":"publication","summary":"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (2) a jailbreaking dataset comprising 100 behaviors -- both original and sourced from prior work (Zou et al., 2023; Mazeika et al., 2023, 2024) -- which align with OpenAI's usage policies; (3) a standardized evaluation framework at [this https URL](https://github.com/JailbreakBench/jailbreakbench) that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at [this https URL](https://jailbreakbench.github.io/) that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community.","tags":[],"title":"JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models","type":"publication"},{"authors":["Fredrik Nestaas","Edoardo Debenedetti","Florian Tramèr"],"categories":[],"content":"","date":1719922628,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719922628,"objectID":"a9a44191e2694631344f591989bd16a2","permalink":"https://edoardo.science/publication/adversarial_seo/","publishdate":"2024-10-20T12:17:08Z","relpermalink":"/publication/adversarial_seo/","section":"publication","summary":"Large Language Models (LLMs) are increasingly used in applications where the model selects from competing third-party content, such as in LLM-powered search engines or chatbot plugins. In this paper, we introduce Preference Manipulation Attacks, a new class of attacks that manipulate an LLM's selections to favor the attacker. We demonstrate that carefully crafted website content or plugin documentations can trick an LLM to promote the attacker products and discredit competitors, thereby increasing user traffic and monetization. We show this leads to a prisoner's dilemma, where all parties are incentivized to launch attacks, but the collective effect degrades the LLM's outputs for everyone. We demonstrate our attacks on production LLM search engines (Bing and Perplexity) and plugin APIs (for GPT-4 and Claude). As LLMs are increasingly used to rank third-party content, we expect Preference Manipulation Attacks to emerge as a significant threat.","tags":[],"title":"Adversarial Search Engine Optimization for Large Language Models","type":"publication"},{"authors":["Xiangyu Qi","Yangsibo Huang","Yi Zeng","Edoardo Debenedetti","Jonas Geiping","Luxi He","Kaixuan Huang","Udari Madhushani","Vikash Sehwag","Weijia Shi","Boyi Wei","Tinghao Xie","Danqi Chen","Pin-Yu Chen","Jeffrey Ding","Ruoxi Jia","Jiaqi Ma","Arvind Narayanan","Weijie J Su","Mengdi Wang","Chaowei Xiao","Bo Li","Dawn Song","Peter Henderson","Prateek Mittal"],"categories":[],"content":"","date":1716967555,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716967555,"objectID":"acc864651c79fca2bdf5dc87b65c81af","permalink":"https://edoardo.science/publication/ai_risk_management/","publishdate":"2024-05-29T07:25:55Z","relpermalink":"/publication/ai_risk_management/","section":"publication","summary":"The exposure of security vulnerabilities in safety-aligned language models, e.g., susceptibility to adversarial attacks, has shed light on the intricate interplay between AI safety and AI security. Although the two disciplines now come together under the overarching goal of AI risk management, they have historically evolved separately, giving rise to differing perspectives. Therefore, in this paper, we advocate that stakeholders in AI risk management should be aware of the nuances, synergies, and interplay between safety and security, and unambiguously take into account the perspectives of both disciplines in order to devise mostly effective and holistic risk mitigation approaches. Unfortunately, this vision is often obfuscated, as the definitions of the basic concepts of \"safety\" and \"security\" themselves are often inconsistent and lack consensus across communities. With AI risk management being increasingly cross-disciplinary, this issue is particularly salient. In light of this conceptual challenge, we introduce a unified reference framework to clarify the differences and interplay between AI safety and AI security, aiming to facilitate a shared understanding and effective collaboration across communities.","tags":[],"title":"AI Risk Management Should Incorporate Both Safety and Security","type":"publication"},{"authors":["Edoardo Debenedetti","Zishen Wan","Maksym Andriushchenko","Vikash Sehwag","Kshitij Bhardwaj","Bhavya Kailkhura"],"categories":[],"content":"","date":1715430146,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715430146,"objectID":"d98ad7fbe05a0a136be6c903cfd45a6a","permalink":"https://edoardo.science/publication/scaling_adv_training/","publishdate":"2024-10-20T12:22:26Z","relpermalink":"/publication/scaling_adv_training/","section":"publication","summary":"The last six years have witnessed significant progress in adversarially robust deep learning. As evidenced by the CIFAR-10 dataset category in RobustBench benchmark, the accuracy under ℓ∞ adversarial perturbations improved from 44% in Madry et al. (2018) to 71% in Peng et al. (2023). Although impressive, existing state-of-the-art is still far from satisfactory. It is further observed that best-performing models are often very large models adversarially trained by industrial labs with significant computational budgets. In this paper, we aim to understand: \"how much longer can computing power drive adversarial robustness advances?\" To answer this question, we derive *scaling laws for adversarial robustness* which can be extrapolated in the future to provide an estimate of how much cost we would need to pay to reach a desired level of robustness. We show that increasing the FLOPs needed for adversarial training does not bring as much advantage as it does for standard training in terms of performance improvements. Moreover, we find that some of the top-performing techniques are difficult to exactly reproduce, suggesting that they are not robust enough for minor changes in the training setup. Our analysis also uncovers potentially worthwhile directions to pursue in future research. Finally, we make our benchmarking framework (built on top of `timm`) publicly available to facilitate future analysis in efficient robust deep learning.","tags":[],"title":"Scaling Compute Is Not All You Need for Adversarial Robustness","type":"publication"},{"authors":["Edoardo Debenedetti","Nicholas Carlini","Florian Tramèr"],"categories":[],"content":"","date":1704555196,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704555196,"objectID":"bf3f4dc9b217df79ec7212471987edae","permalink":"https://edoardo.science/publication/breaking_eggs/","publishdate":"2023-06-06T15:33:16Z","relpermalink":"/publication/breaking_eggs/","section":"publication","summary":"We propose a new real-world oriented metric for black-box decision-based attacks on security-critical systems","tags":["Distinguished Paper Award Runner-up"],"title":"Evading Black-box Classifiers Without Breaking Eggs","type":"publication"},{"authors":["Edoardo Debenedetti","Giorgio Severi","Nicholas Carlini","Christopher A. Choquette-Choo","Matthew Jagielski","Milad Nasr","Eric Wallace","Florian Tramèr"],"categories":[],"content":"","date":1694521336,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694521336,"objectID":"5fb0911bac2af8379bcfddc5cb52f369","permalink":"https://edoardo.science/publication/side_channels/","publishdate":"2023-09-12T12:22:16Z","relpermalink":"/publication/side_channels/","section":"publication","summary":"Most current approaches for protecting privacy in machine learning (ML) assume that models exist in a vacuum. Yet, in reality, these models are part of larger systems that include components for training data filtering, output monitoring, and more. In this work, we introduce privacy side channels: attacks that exploit these system-level components to extract private information at far higher rates than is otherwise possible for standalone models. We propose four categories of side channels that span the entire ML lifecycle (training data filtering, input preprocessing, output post-processing, and query filtering) and allow for enhanced membership inference, data extraction, and even novel threats such as extraction of users' test queries. For example, we show that deduplicating training data before applying differentially-private training creates a side-channel that completely invalidates any provable privacy guarantees. We further show that systems which block language models from regenerating training data can be exploited to exfiltrate private keys contained in the training set--even if the model did not memorize these keys. Taken together, our results demonstrate the need for a holistic, end-to-end privacy analysis of machine learning systems.","tags":[],"title":"Privacy Side Channels in Machine Learning Systems","type":"publication"},{"authors":["Edoardo Debenedetti","Vikash Sehwag","Prateek Mittal"],"categories":[],"content":"","date":1663687996,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663687996,"objectID":"fbb2b56df6505044500f810a7c069bff","permalink":"https://edoardo.science/publication/light_recipe/","publishdate":"2022-09-20T15:33:16Z","relpermalink":"/publication/light_recipe/","section":"publication","summary":"This paper shows that ViTs are highly suitable for adversarial training to achieve competitive performance and recommends that the community should avoid translating the canonical training recipes in ViTs to robust training and rethink common training choices in the context of adversarial training.","tags":[],"title":"A Light Recipe to Train Robust Vision Transformers","type":"publication"},{"authors":["Edoardo Debenedetti"],"categories":[],"content":"","date":1652358837,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652358837,"objectID":"0f40cf545adf79432be77d9037b7794b","permalink":"https://edoardo.science/publication/thesis/","publishdate":"2022-05-12T12:33:57Z","relpermalink":"/publication/thesis/","section":"publication","summary":"My Master's thesis done at EPFL under the supervision of Princeton's Vikash Sehwag and Prof. Prateek Mittal, and EPFL's Prof. Troncoso. We show that we can obtain state of the art results in adversarial training using Vision Transformers (in particular with XCiT) on ImageNet.","tags":[],"title":"Adversarially Robust Vision Transformers","type":"publication"},{"authors":["Francesco Croce","Maksym Andriushchenko","Vikash Sehwag","Edoardo Debenedetti","Nicolas Flammarion","Mung Chiang","Prateek Mittal","Matthias Hein"],"categories":[],"content":"","date":1617007444,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617007444,"objectID":"2c2bf09e35bb19092bef162197364a73","permalink":"https://edoardo.science/publication/robustbench/","publishdate":"2021-03-29T10:44:04+02:00","relpermalink":"/publication/robustbench/","section":"publication","summary":"As a research community, we are still lacking a systematic understanding of the progress on adversarial robustness which often makes it hard to identify the most promising ideas in training robust models. A key challenge in benchmarking robustness is that its evaluation is often error-prone leading to robustness overestimation. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classification task and introduce restrictions (possibly loosened in the future) on the allowed models and evaluate adversarial robustness with AutoAttack, an ensemble of white- and black-box attacks, which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. To prevent overadaptation of new defenses to AutoAttack, we welcome external evaluations based on adaptive attacks, especially where AutoAttack flags a potential overestimation of robustness. Our leaderboard, hosted at https://robustbench.github.io/, contains evaluations of 120+ models and aims at reflecting the current state of the art in image classification on a set of well-defined tasks in $\\ell_2$- and $\\ell_\\infty$-threat models and on common corruptions, with possible extensions in the future. Additionally, we open-source the library https://github.com/RobustBench/robustbench that provides unified access to 80+ robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze the impact of robustness on the performance on distribution shifts, calibration, out-of-distribution detection, fairness, privacy leakage, smoothness, and transferability. ","tags":[],"title":"RobustBench: A standardized benchmark for adversarial robustness","type":"publication"},{"authors":null,"categories":null,"content":"\r[07/2025 - Meta Internship] In July, I will start an internship in the GenAI Red Team at Meta.\n[04/2025 - SafeBench Prize] AgentDojo got a SafeBench First prize, worth USD 50\u0026#39;000.\n[04/2025 - CaMeL is out!] The paper resulting from my intership at Google is finally out. We propose a new method, to solve prompt injections by design.\n[10/2024 - Internship] On October 1st I started as Student Researcher in the AI Red Team at Google. I will be working with Tianqi Fan and Ilia Shumailov (Google DeepMind) on AI agents security.\n[09/2024 - Spotlight] The report of our SaTML LLMs CTF has been accepted as spotlight at the NeurIPS D\u0026amp;B Track 2024! Also AgentDojo and JailbreakBench were accepted.\n[04/2024 - Award] Evading Black-box Classifiers Without Breaking Eggs, selected as Distinguished Paper Award Runner-up at IEEE SaTML 2024!\n[04/2024 - New Paper: JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models] We have a new paper about benchmarking LLM jailbreak attacks and defenses with a focus on transparency and reproducibility. Take a look here.\n[12/2023 - SaTML 2024 news] Presenting Evading Black-box Classifiers Without Breaking Eggs, and co-organizing the LLMs CTF.\n[06/2023 - New paper: Privacy Side Channels in Machine Learning Systems] We have a new paper, about side-channels in ML systems, i.e., by exploiting components other than the model. Spoiler alert: some of those components, on paper, are meant to improve privacy! Take a look here.\n[06/2023 - New paper: Evading Black-box Classifiers Without Breaking Eggs] We uploaded on arXiv a new paper, where we propose a new real-world oriented metric for black-box decision-based attacks on security-critical systems. Take a look here!\n[11/2022 - A Light Recipe to Train Robust Vision Transformers accepted at SaTML] The paper derived from my master thesis was accepted at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML 2023).\n[09/2022 - New paper: A Light Recipe to Train Robust Vision Transformers] We uploaded on arXiv the paper derived from my Master’s thesis, with additional experiments and insights. Take a look here.\n[08/2022 - I started my PhD] On August 1st, 2022, I started my PhD at ETH Zürich, in the Privacy and Security Lab of Prof. Florian Tramèr.\n[12/05/2022 - I earned my MSc at EPFL!] On April 27th I successfully defended my MSc thesis about Adversarially Robust Vision Transformers! You can read it here. Feel free to contact me if you have any questions about it!\n","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"a0812ae5f3c926fea6faf4472cefc8e2","permalink":"https://edoardo.science/news/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/news/","section":"","summary":"List of news.\r\n","tags":[],"title":"News","type":"page"},{"authors":null,"categories":null,"content":"My current work is around the security of AI agents. Some of my recent work in this area includes:\nCaMeL: a system-level prompt injection defense that virtually solves the security issue of tool-calling AI agents by design. AgentDojo: a benchmark for prompt injection attacks and defenses. Adversarial SEO for LLMs: we showed that you can use prompt-injection attacks to promote your own webpages in LLM-based search engines like Perplexity AI. AutoAdvExBench: a benchmark to measure how good LLMs are at breaking adversarial example defenses, as a way to measure how good LLMs are at doing (ML) security research. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e352231bc94d07e7f5edb0bd0a11ef49","permalink":"https://edoardo.science/current-work/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/current-work/","section":"","summary":"My current work is around the security of AI agents. Some of my recent work in this area includes:\nCaMeL: a system-level prompt injection defense that virtually solves the security issue of tool-calling AI agents by design.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"[07/2025 - Meta Internship] In July, I will start an internship in the GenAI Red Team at Meta.\n[04/2025 - SafeBench Prize] AgentDojo got a SafeBench First prize, worth USD 50\u0026#39;000.\n[04/2025 - CaMeL is out!] The paper resulting from my intership at Google is finally out. We propose a new method, to solve prompt injections by design.\n[10/2024 - Internship] On October 1st I started as Student Researcher in the AI Red Team at Google. I will be working with Tianqi Fan and Ilia Shumailov (Google DeepMind) on AI agents security.\n[09/2024 - Spotlight] The report of our SaTML LLMs CTF has been accepted as spotlight at the NeurIPS D\u0026amp;B Track 2024! Also AgentDojo and JailbreakBench were accepted.\n[04/2024 - Award] Evading Black-box Classifiers Without Breaking Eggs, selected as Distinguished Paper Award Runner-up at IEEE SaTML 2024!\n[04/2024 - New Paper: JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models] We have a new paper about benchmarking LLM jailbreak attacks and defenses with a focus on transparency and reproducibility. Take a look here.\n[12/2023 - SaTML 2024 news] Presenting Evading Black-box Classifiers Without Breaking Eggs, and co-organizing the LLMs CTF.\n[06/2023 - New paper: Privacy Side Channels in Machine Learning Systems] We have a new paper, about side-channels in ML systems, i.e., by exploiting components other than the model. Spoiler alert: some of those components, on paper, are meant to improve privacy! Take a look here.\n[06/2023 - New paper: Evading Black-box Classifiers Without Breaking Eggs] We uploaded on arXiv a new paper, where we propose a new real-world oriented metric for black-box decision-based attacks on security-critical systems. Take a look here!\n[11/2022 - A Light Recipe to Train Robust Vision Transformers accepted at SaTML] The paper derived from my master thesis was accepted at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML 2023).\n[09/2022 - New paper: A Light Recipe to Train Robust Vision Transformers] We uploaded on arXiv the paper derived from my Master’s thesis, with additional experiments and insights. Take a look here.\n[08/2022 - I started my PhD] On August 1st, 2022, I started my PhD at ETH Zürich, in the Privacy and Security Lab of Prof. Florian Tramèr.\n[12/05/2022 - I earned my MSc at EPFL!] On April 27th I successfully defended my MSc thesis about Adversarially Robust Vision Transformers! You can read it here. Feel free to contact me if you have any questions about it!\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4ba0b231c7eeb0b6fffbbe093c76caa6","permalink":"https://edoardo.science/newslist/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/newslist/","section":"","summary":"[07/2025 - Meta Internship] In July, I will start an internship in the GenAI Red Team at Meta.\n[04/2025 - SafeBench Prize] AgentDojo got a SafeBench First prize, worth USD 50'000.","tags":null,"title":"","type":"page"}]