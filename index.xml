<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Edoardo Debenedetti</title><link>https://edoardo.science/</link><atom:link href="https://edoardo.science/index.xml" rel="self" type="application/rss+xml"/><description>Edoardo Debenedetti</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 12 Dec 2024 12:36:17 +0000</lastBuildDate><image><url>https://edoardo.science/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url><title>Edoardo Debenedetti</title><link>https://edoardo.science/</link></image><item><title>AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents</title><link>https://edoardo.science/publication/agentdojo/</link><pubDate>Thu, 12 Dec 2024 12:36:17 +0000</pubDate><guid>https://edoardo.science/publication/agentdojo/</guid><description/></item><item><title>Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition</title><link>https://edoardo.science/publication/ctf_report/</link><pubDate>Thu, 12 Dec 2024 12:31:48 +0000</pubDate><guid>https://edoardo.science/publication/ctf_report/</guid><description/></item><item><title>JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models</title><link>https://edoardo.science/publication/jailbreakbench/</link><pubDate>Mon, 09 Dec 2024 14:11:22 +0000</pubDate><guid>https://edoardo.science/publication/jailbreakbench/</guid><description/></item><item><title>Adversarial Search Engine Optimization for Large Language Models</title><link>https://edoardo.science/publication/adversarial_seo/</link><pubDate>Tue, 02 Jul 2024 12:17:08 +0000</pubDate><guid>https://edoardo.science/publication/adversarial_seo/</guid><description/></item><item><title>AI Risk Management Should Incorporate Both Safety and Security</title><link>https://edoardo.science/publication/ai_risk_management/</link><pubDate>Wed, 29 May 2024 07:25:55 +0000</pubDate><guid>https://edoardo.science/publication/ai_risk_management/</guid><description/></item><item><title>Scaling Compute Is Not All You Need for Adversarial Robustness</title><link>https://edoardo.science/publication/scaling_adv_training/</link><pubDate>Sat, 11 May 2024 12:22:26 +0000</pubDate><guid>https://edoardo.science/publication/scaling_adv_training/</guid><description/></item><item><title>Evading Black-box Classifiers Without Breaking Eggs</title><link>https://edoardo.science/publication/breaking_eggs/</link><pubDate>Sat, 06 Jan 2024 15:33:16 +0000</pubDate><guid>https://edoardo.science/publication/breaking_eggs/</guid><description/></item><item><title>Privacy Side Channels in Machine Learning Systems</title><link>https://edoardo.science/publication/side_channels/</link><pubDate>Tue, 12 Sep 2023 12:22:16 +0000</pubDate><guid>https://edoardo.science/publication/side_channels/</guid><description/></item><item><title>A Light Recipe to Train Robust Vision Transformers</title><link>https://edoardo.science/publication/light_recipe/</link><pubDate>Tue, 20 Sep 2022 15:33:16 +0000</pubDate><guid>https://edoardo.science/publication/light_recipe/</guid><description/></item><item><title>Adversarially Robust Vision Transformers</title><link>https://edoardo.science/publication/thesis/</link><pubDate>Thu, 12 May 2022 12:33:57 +0000</pubDate><guid>https://edoardo.science/publication/thesis/</guid><description/></item><item><title>RobustBench: A standardized benchmark for adversarial robustness</title><link>https://edoardo.science/publication/robustbench/</link><pubDate>Mon, 29 Mar 2021 10:44:04 +0200</pubDate><guid>https://edoardo.science/publication/robustbench/</guid><description/></item><item><title>News</title><link>https://edoardo.science/news/</link><pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate><guid>https://edoardo.science/news/</guid><description>
&lt;p>&lt;strong>[07/2025 - Meta Internship]&lt;/strong> In July, I will start an internship in the GenAI Red Team at Meta.&lt;/p>
&lt;p>&lt;strong>[04/2025 - SafeBench Prize]&lt;/strong> &lt;a href="https://agentdojo.spylab.ai" target="_blank" rel="noopener">AgentDojo&lt;/a> got a &lt;a href="https://www.mlsafety.org/safebench" target="_blank" rel="noopener">SafeBench&lt;/a> First prize, worth USD 50'000.&lt;/p>
&lt;p>&lt;strong>[04/2025 - CaMeL is out!]&lt;/strong> The paper resulting from my intership at Google is finally &lt;a href="https://arxiv.org/abs/2503.18813" target="_blank" rel="noopener">out&lt;/a>. We propose a new method, to solve prompt injections &lt;em>by design&lt;/em>.&lt;/p>
&lt;p>&lt;strong>[10/2024 - Internship]&lt;/strong> On October 1st I started as Student Researcher in the AI Red Team at Google. I will be working with Tianqi Fan and &lt;a href="https://www.cl.cam.ac.uk/~is410/" target="_blank" rel="noopener">Ilia Shumailov&lt;/a> (Google DeepMind) on AI agents security.&lt;/p>
&lt;p>&lt;strong>[09/2024 - Spotlight]&lt;/strong> The &lt;a href="https://arxiv.org/abs/2406.07954" target="_blank" rel="noopener">report&lt;/a> of our SaTML LLMs CTF has been accepted as &lt;strong>spotlight&lt;/strong> at the NeurIPS D&amp;amp;B Track 2024! Also &lt;a href="https://agentdojo.spylab.ai" target="_blank" rel="noopener">AgentDojo&lt;/a> and &lt;a href="https://jailbreakbench.github.io" target="_blank" rel="noopener">JailbreakBench&lt;/a> were accepted.&lt;/p>
&lt;p>&lt;strong>[04/2024 - Award]&lt;/strong> &lt;em>&lt;a href="https://arxiv.org/abs/2306.02895" target="_blank" rel="noopener">Evading Black-box Classifiers Without Breaking Eggs&lt;/a>&lt;/em>, selected as Distinguished Paper Award Runner-up at IEEE SaTML 2024!&lt;/p>
&lt;p>&lt;strong>[04/2024 - New Paper: &lt;em>JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models&lt;/em>]&lt;/strong> We have a new paper about benchmarking LLM jailbreak attacks and defenses with a focus on transparency and reproducibility. Take a look &lt;a href="https://arxiv.org/abs/2404.01318" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[12/2023 - SaTML 2024 news]&lt;/strong> Presenting &lt;em>&lt;a href="https://arxiv.org/abs/2306.02895" target="_blank" rel="noopener">Evading Black-box Classifiers Without Breaking Eggs&lt;/a>&lt;/em>, and co-organizing the &lt;a href="https://ctf.spylab.ai" target="_blank" rel="noopener">LLMs CTF&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[06/2023 - New paper: &lt;em>Privacy Side Channels in Machine Learning Systems&lt;/em>]&lt;/strong> We have a new paper, about side-channels in ML &lt;em>systems&lt;/em>, i.e., by exploiting components other than the model. Spoiler alert: some of those components, on paper, are meant to &lt;strong>improve&lt;/strong> privacy! Take a look &lt;a href="https://arxiv.org/abs/2309.05610" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[06/2023 - New paper: &lt;em>Evading Black-box Classifiers Without Breaking Eggs&lt;/em>]&lt;/strong> We uploaded on arXiv a new paper, where we propose a new real-world oriented metric for black-box decision-based attacks on security-critical systems. Take a look &lt;a href="https://arxiv.org/abs/2306.02895" target="_blank" rel="noopener">here&lt;/a>!&lt;/p>
&lt;p>&lt;strong>[11/2022 - &lt;em>A Light Recipe to Train Robust Vision Transformers&lt;/em> accepted at SaTML]&lt;/strong> The paper derived from my master thesis was accepted at the &lt;a href="https://satml.org" target="_blank" rel="noopener">IEEE Conference on Secure and Trustworthy Machine Learning&lt;/a> (SaTML 2023).&lt;/p>
&lt;p>&lt;strong>[09/2022 - New paper: &lt;em>A Light Recipe to Train Robust Vision Transformers&lt;/em>]&lt;/strong> We uploaded on arXiv the paper derived from my Master&amp;rsquo;s thesis, with additional experiments and insights. Take a look &lt;a href="https://arxiv.org/abs/2209.07399" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[08/2022 - I started my PhD]&lt;/strong> On August 1st, 2022, I started my PhD at ETH Zürich, in the Privacy and Security Lab of Prof. &lt;a href="https://floriantramer.com" target="_blank" rel="noopener">Florian Tramèr&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[12/05/2022 - I earned my MSc at EPFL!]&lt;/strong> On April 27th I successfully defended my MSc thesis about Adversarially Robust Vision Transformers! You can read it &lt;a href="https://edoardo.science/publication/thesis/">here&lt;/a>. Feel free to contact me if you have any questions about it!&lt;/p></description></item><item><title/><link>https://edoardo.science/current-work/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edoardo.science/current-work/</guid><description>&lt;p>My current work is around the security of AI agents. Some of my recent work in this area includes:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://arxiv.org/abs/2503.18813" target="_blank" rel="noopener">CaMeL&lt;/a>: a system-level prompt injection defense that virtually solves the security issue of tool-calling AI agents &lt;em>by design&lt;/em>.&lt;/li>
&lt;li>&lt;a href="https://agentdojo.spylab.ai" target="_blank" rel="noopener">AgentDojo&lt;/a>: a benchmark for prompt injection attacks and defenses.&lt;/li>
&lt;li>&lt;a href="https://openreview.net/forum?id=hkdqxN3c7t" target="_blank" rel="noopener">Adversarial SEO for LLMs&lt;/a>: we showed that you can use prompt-injection attacks to promote your own webpages in LLM-based search engines like Perplexity AI.&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2503.01811" target="_blank" rel="noopener">AutoAdvExBench&lt;/a>: a benchmark to measure how good LLMs are at breaking adversarial example defenses, as a way to measure how good LLMs are at doing (ML) security research.&lt;/li>
&lt;/ul></description></item><item><title/><link>https://edoardo.science/newslist/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edoardo.science/newslist/</guid><description>&lt;p>&lt;strong>[07/2025 - Meta Internship]&lt;/strong> In July, I will start an internship in the GenAI Red Team at Meta.&lt;/p>
&lt;p>&lt;strong>[04/2025 - SafeBench Prize]&lt;/strong> &lt;a href="https://agentdojo.spylab.ai" target="_blank" rel="noopener">AgentDojo&lt;/a> got a &lt;a href="https://www.mlsafety.org/safebench" target="_blank" rel="noopener">SafeBench&lt;/a> First prize, worth USD 50'000.&lt;/p>
&lt;p>&lt;strong>[04/2025 - CaMeL is out!]&lt;/strong> The paper resulting from my intership at Google is finally &lt;a href="https://arxiv.org/abs/2503.18813" target="_blank" rel="noopener">out&lt;/a>. We propose a new method, to solve prompt injections &lt;em>by design&lt;/em>.&lt;/p>
&lt;p>&lt;strong>[10/2024 - Internship]&lt;/strong> On October 1st I started as Student Researcher in the AI Red Team at Google. I will be working with Tianqi Fan and &lt;a href="https://www.cl.cam.ac.uk/~is410/" target="_blank" rel="noopener">Ilia Shumailov&lt;/a> (Google DeepMind) on AI agents security.&lt;/p>
&lt;p>&lt;strong>[09/2024 - Spotlight]&lt;/strong> The &lt;a href="https://arxiv.org/abs/2406.07954" target="_blank" rel="noopener">report&lt;/a> of our SaTML LLMs CTF has been accepted as &lt;strong>spotlight&lt;/strong> at the NeurIPS D&amp;amp;B Track 2024! Also &lt;a href="https://agentdojo.spylab.ai" target="_blank" rel="noopener">AgentDojo&lt;/a> and &lt;a href="https://jailbreakbench.github.io" target="_blank" rel="noopener">JailbreakBench&lt;/a> were accepted.&lt;/p>
&lt;p>&lt;strong>[04/2024 - Award]&lt;/strong> &lt;em>&lt;a href="https://arxiv.org/abs/2306.02895" target="_blank" rel="noopener">Evading Black-box Classifiers Without Breaking Eggs&lt;/a>&lt;/em>, selected as Distinguished Paper Award Runner-up at IEEE SaTML 2024!&lt;/p>
&lt;p>&lt;strong>[04/2024 - New Paper: &lt;em>JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models&lt;/em>]&lt;/strong> We have a new paper about benchmarking LLM jailbreak attacks and defenses with a focus on transparency and reproducibility. Take a look &lt;a href="https://arxiv.org/abs/2404.01318" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[12/2023 - SaTML 2024 news]&lt;/strong> Presenting &lt;em>&lt;a href="https://arxiv.org/abs/2306.02895" target="_blank" rel="noopener">Evading Black-box Classifiers Without Breaking Eggs&lt;/a>&lt;/em>, and co-organizing the &lt;a href="https://ctf.spylab.ai" target="_blank" rel="noopener">LLMs CTF&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[06/2023 - New paper: &lt;em>Privacy Side Channels in Machine Learning Systems&lt;/em>]&lt;/strong> We have a new paper, about side-channels in ML &lt;em>systems&lt;/em>, i.e., by exploiting components other than the model. Spoiler alert: some of those components, on paper, are meant to &lt;strong>improve&lt;/strong> privacy! Take a look &lt;a href="https://arxiv.org/abs/2309.05610" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[06/2023 - New paper: &lt;em>Evading Black-box Classifiers Without Breaking Eggs&lt;/em>]&lt;/strong> We uploaded on arXiv a new paper, where we propose a new real-world oriented metric for black-box decision-based attacks on security-critical systems. Take a look &lt;a href="https://arxiv.org/abs/2306.02895" target="_blank" rel="noopener">here&lt;/a>!&lt;/p>
&lt;p>&lt;strong>[11/2022 - &lt;em>A Light Recipe to Train Robust Vision Transformers&lt;/em> accepted at SaTML]&lt;/strong> The paper derived from my master thesis was accepted at the &lt;a href="https://satml.org" target="_blank" rel="noopener">IEEE Conference on Secure and Trustworthy Machine Learning&lt;/a> (SaTML 2023).&lt;/p>
&lt;p>&lt;strong>[09/2022 - New paper: &lt;em>A Light Recipe to Train Robust Vision Transformers&lt;/em>]&lt;/strong> We uploaded on arXiv the paper derived from my Master&amp;rsquo;s thesis, with additional experiments and insights. Take a look &lt;a href="https://arxiv.org/abs/2209.07399" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[08/2022 - I started my PhD]&lt;/strong> On August 1st, 2022, I started my PhD at ETH Zürich, in the Privacy and Security Lab of Prof. &lt;a href="https://floriantramer.com" target="_blank" rel="noopener">Florian Tramèr&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[12/05/2022 - I earned my MSc at EPFL!]&lt;/strong> On April 27th I successfully defended my MSc thesis about Adversarially Robust Vision Transformers! You can read it &lt;a href="https://edoardo.science/publication/thesis/">here&lt;/a>. Feel free to contact me if you have any questions about it!&lt;/p></description></item></channel></rss>